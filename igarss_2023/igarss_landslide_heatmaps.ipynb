{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Landslide Hazard Analysis Using SAR\n",
    "Intro to the use case/methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Search for SLC images over Haiti in a defined time period"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2017-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-08-16T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-08-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "import asf_search\n",
    "scenes_to_submit = []\n",
    "wkt = 'POLYGON((-74.6 18.0,-73.0 18.0,-73.0 18.8,-74.6 18.8, -74.6 18.0))'\n",
    "results = asf_search.geo_search(platform=[asf_search.PLATFORM.SENTINEL1], intersectsWith=wkt, processingLevel='SLC', start=start_time, end=end_time)\n",
    "[scenes_to_submit.append(result.properties['sceneName']) for result in results]\n",
    "print(f'There are {len(scenes_to_submit)} scenes in the AOI between {start_time} and {end_time}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Request HyP3 processing of Haiti data for two years"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "\n",
    "print(f'Submitting {len(scenes_to_submit)} jobs')\n",
    "hyp3 = sdk.HyP3()\n",
    "rtc_jobs = sdk.Batch()\n",
    "for scene in scenes_to_submit:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(granule=scene, name='IGARSS-HAITI')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rtc_jobs = hyp3.watch(rtc_jobs)\n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "file_list = succeeded_jobs.download_files()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the staged version of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "for file in file_list:\n",
    "    download_path = Path(file)\n",
    "    s3_client.download_file('jrsmale-shenanigans', f'igarss_2023/{file}', download_path)\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(download_path.stem)\n",
    "    os.unlink(download_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Crop all images to same extent (e.g. InSAR Notebook)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "def clip_hyp3_products_to_same_extent(data_dir: Union[str, Path], extent: List[float]) -> List[str]:\n",
    "    \"\"\"Clip all GeoTIFF hyp3 products to a specified extent\n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        extent:\n",
    "            a list of the upper-left x, upper-left y, lower-right x, and lower-right y corner coordinates of desired extent.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    files_for_xarray = ['VV.tif', 'VH.tif', '_ls_map.tif']\n",
    "\n",
    "    for extension in files_for_xarray:\n",
    "        print(extension)\n",
    "        for file in data_dir.glob(f'**/*{extension}'):\n",
    "            dst_file = Path(str(file.absolute()).replace(f'{extension}', f'{extension[:-4]}_clipped.tif'))\n",
    "            if not Path(dst_file).is_file():\n",
    "                gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=extent)#, projWinSRS=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/jrsmale/data/unzipped_IGARSS/')\n",
    "extent = [544823, 2071376, 683411, 1987136] #TODO: same/similar coords different proj because gdal didn't like EPSG:4326\n",
    "clip_hyp3_products_to_same_extent(data_dir, extent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the geotiffs into Xarray with datetime stamps for each image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tif_names(scene_dir_path):\n",
    "    scene_path_listed = os.listdir(scene_dir_path)\n",
    "    scene_files_vv = [fname for fname in scene_path_listed if fname.endswith('_VV_clipped.tif')]\n",
    "    scene_files_vh = [fname for fname in scene_path_listed if fname.endswith('_VH_clipped.tif')]\n",
    "    scene_files_ls = [fname for fname in scene_path_listed if fname.endswith('_ls_map_clipped.tif')]\n",
    "    scene_file_rm = [fname for fname in scene_path_listed if fname.endswith('README.md.txt')]\n",
    "    return scene_files_vv, scene_files_vh, scene_files_ls, scene_file_rm\n",
    "\n",
    "scenes_listed = os.listdir(data_dir)\n",
    "fpaths_vv, fpaths_vh, fpaths_ls, fpaths_rm = [],[],[], []\n",
    "\n",
    "for element in range(len(scenes_listed)):\n",
    "    print(f'{data_dir}/{scenes_listed[element]}')\n",
    "    try:\n",
    "        good_files = extract_tif_names(f'{data_dir}/{scenes_listed[element]}')\n",
    "        path_vh = f'{data_dir}/{scenes_listed[element]}/{good_files[0][0]}'\n",
    "        path_vv = f'{data_dir}/{scenes_listed[element]}/{good_files[1][0]}'\n",
    "        path_ls = f'{data_dir}/{scenes_listed[element]}/{good_files[2][0]}'\n",
    "        path_readme = f'{data_dir}/{scenes_listed[element]}/{good_files[3][0]}'\n",
    "\n",
    "        fpaths_vv.append(path_vv)\n",
    "        fpaths_vh.append(path_vh)\n",
    "        fpaths_ls.append(path_ls)\n",
    "        fpaths_rm.append(path_readme)\n",
    "    except:\n",
    "        print(f'Couldnt use {scenes_listed[element]}. Skipping.')\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-temporal averaging, create time series plot of single pixel (or group of pixels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "def preprocess(da_orig, file_type):\n",
    "    '''function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI? '''\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({'band_data': file_type}).squeeze()\n",
    "    fname = os.path.basename(da_orig['band_data'].encoding['source'])\n",
    "\n",
    "    sensor = fname[0:3]\n",
    "    beam_mode = fname[4:6]\n",
    "    acq_date_raw = fname[7:22]  # need to parse further\n",
    "    acq_date = datetime.strptime(acq_date_raw, '%Y%m%dT%H%M%S')\n",
    "    acq_time = fname[15:22]\n",
    "    pol_type = fname[24:25]  # dual pol ...\n",
    "    primary_pol = fname[25:26]  # VV ...\n",
    "    orbit_type = fname[26:27]  # Precise (P), Restituted (R), or Original Predicted (O)\n",
    "    terrain_correction_pixel_spacing = fname[27:32]  # Terrain Correction Pixel Spacing\n",
    "    output = fname[35]  # Gamma-0 (g) or Sigma-0 (s) Output\n",
    "    output_type = fname[36]  # Power (p) or Decibel (d) or Amplitude (a) Output\n",
    "    masked = fname[37]  # Unmasked (u) or Water Masked (w)\n",
    "    filtered = fname[38]  # Not Filtered (n) or Filtered (f)\n",
    "    area = fname[39]  # Entire Area (e) or Clipped Area (c)\n",
    "    product_id = fname[42:46]  # Product ID\n",
    "\n",
    "    da.attrs = {'sensor': sensor,\n",
    "                  'beam_mode': beam_mode,\n",
    "                  'acquisition_date': acq_date,\n",
    "                  'acquisition_time': acq_time,\n",
    "                  'polarisation_type': pol_type,\n",
    "                  'primary_polarisation': primary_pol,\n",
    "                  'orbit_type': orbit_type,\n",
    "                  'terrain_correction_pixel_spacing': terrain_correction_pixel_spacing,\n",
    "                  'output_format': output,\n",
    "                  'output_type': output_type,\n",
    "                  'masked': masked,\n",
    "                  'filtered': filtered,\n",
    "                  'area': area,\n",
    "                  'product_id': product_id\n",
    "                  }\n",
    "    utm_zone = da.spatial_ref.attrs['crs_wkt'][17:29]\n",
    "    epsg_code = da.spatial_ref.attrs['crs_wkt'][589:594]\n",
    "\n",
    "    da.attrs['utm_zone'] = utm_zone\n",
    "    da.attrs['epsg_code'] = f'EPSG:{epsg_code}'\n",
    "\n",
    "    date = da.attrs['acquisition_date']\n",
    "\n",
    "    da = da.assign_coords({'acq_date': date})\n",
    "    da = da.expand_dims('acq_date')\n",
    "    da = da.drop_duplicates(dim=['x', 'y'])\n",
    "\n",
    "    return da\n",
    "\n",
    "def preprocess_vv(data):\n",
    "    return preprocess(data, file_type='vv')\n",
    "\n",
    "def preprocess_vh(data):\n",
    "    return preprocess(data, file_type='vh')\n",
    "\n",
    "def preprocess_ls(data):\n",
    "    return preprocess(data, file_type='ls')\n",
    "\n",
    "asf_vv = xr.open_mfdataset(paths = fpaths_vh, preprocess = preprocess_vv, chunks = 'auto', engine='rasterio', data_vars='minimal', coords='minimal', concat_dim='acq_date', combine='nested', parallel=True)\n",
    "asf_vh = xr.open_mfdataset(paths = fpaths_vh, preprocess = preprocess_vh, chunks = 'auto', engine='rasterio', data_vars='minimal', coords='minimal', concat_dim='acq_date', combine='nested', parallel=True)\n",
    "asf_ls = xr.open_mfdataset(paths = fpaths_vh, preprocess = preprocess_ls, chunks = 'auto', engine='rasterio', data_vars='minimal', coords='minimal', concat_dim='acq_date', combine='nested', parallel=True)\n",
    "\n",
    "#TODO: COMBINE DATASET OR REMOVE UNNCECESARRY FILE TYPES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a temporal average for both pre and post EQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def power_to_db(input_arr):\n",
    "    return 10*np.log10(np.abs(input_arr))\n",
    "\n",
    "date_bins = [start_time, event_time, end_time]\n",
    "date_bin_labels = [\"preevent\", \"postevent\"]\n",
    "\n",
    "asf_vv = asf_vv.groupby_bins(\"acq_date\", date_bins, labels=date_bin_labels)\n",
    "asf_vh = asf_vh.groupby_bins(\"acq_date\", date_bins, labels=date_bin_labels)\n",
    "asf_ls = asf_ls.groupby_bins(\"acq_date\", date_bins, labels=date_bin_labels)\n",
    "\n",
    "mean_vv = asf_vv.mean(dim='acq_date')\n",
    "mean_vh = asf_vh.mean(dim='acq_date')\n",
    "mean_ls = asf_vv.mean(dim='acq_date')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform a log difference of the two resulting images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_preevent = np.log10(mean_vv['vv'][0])\n",
    "log_postevent = np.log10(mean_vv['vv'][1])\n",
    "\n",
    "log_diff = log_postevent - log_preevent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discuss options for thresholding the difference image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the COP30 DEM to remove change detections in areas with slopes <5 degrees use gdaldem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from asf_tools.dem import prepare_dem_vrt\n",
    "from osgeo import ogr\n",
    "\n",
    "dem_file = data_dir / 'DEM.tif'\n",
    "prepare_dem_vrt(dem_file, ogr.CreateGeometryFromWkt(wkt))\n",
    "process_dem_file = data_dir / 'DEM_processed.tif'\n",
    "gdal.DEMProcessing(destName=str(process_dem_file), srcDS=str(dem_file), processing=\"slope\", format=\"Gtiff\", slopeFormat=\"degree\")\n",
    "\n",
    "dem = gdal.Open(str(process_dem_file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Plot a histogram for the difference image (might have to subset histogram source to area with equal amounts of landslides and non-landslides)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a figure to interactively view the data and set a threshold using matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}