{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Landslide Hazard Analysis Using SAR\n",
    "In this notebook, you will learn how to generate a landslide density map  with RTC-based change detection analysis.  You will create multi-temporal stacks of Sentinel-1 data using [XArray](https://docs.xarray.dev/en/stable/) and compare data from before and after an earthquake that triggered landslides.\n",
    "\n",
    "In this tutorial, we will focus mapping landslides triggered by an $M_w$ 7.2 [earthquake in Haiti](https://en.wikipedia.org/wiki/2021_Haiti_earthquake) that occurred on August 14, 2021. The earthquake triggered widespread landslides across the southwestern part of the country, with  least 8,444 landslides triggered across a 2,700 $km^2$ (1,000 sq mi) area [(Zhang et al., 2021)](https://www.sciencedirect.com/science/article/abs/pii/S0169555X22003129).  Rapid response to the landslide and inventory of landslides was further hindered by the arrival of Tropical Storm Grace. The presence of consistent cloud coverage makes SAR data an ideal candidate to detect landslides in the aftermath, since SAR can penetrate through cloud cover.\n",
    "\n",
    "This notebook will show you how to perform the time-series change detection as performed by [Handwerger et al., 2022](https://nhess.copernicus.org/articles/22/753/2022/). This process utilizes OnDemand RTC products from the Alaska Satellite Facility.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Use the HyP3 Python SDK to:\n",
    "   - Request On Demand RTC products from ASF HyP3\n",
    "   - Download the RTC products when they are done processing\n",
    "<br></br>\n",
    "2. And you will use [Xarray](https://docs.xarray.dev/en/stable/) and various Python utilities to:\n",
    "   - Download pre-processed data that we will use for the time series\n",
    "   - Load data with Xarray\n",
    "   - Group data pre-event and post-event and average both stacks\n",
    "   - Perform a log difference between pre-event and post-even scenes\n",
    "   - Plot a histogram of the log difference for a subset of the area\n",
    "   - Reproduce the main landslide heatmap figure from Handwerger et al., 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Initial Setup\n",
    "\n",
    "To run this notebook, you'll need to be in the `insar_analysis` conda environment within OpenSARLab.\n",
    "\n",
    "Alternatively, you can set up your own environment by running these commands in your shell (you'll need to have [conda](https://docs.conda.io/projects/continuumio-conda/en/latest/user-guide/install/index.html) installed):\n",
    "```shell\n",
    "curl -OL https://raw.githubusercontent.com/ASFOpenSARlab/opensarlab-envs/main/Environment_Configs/insar_analysis_env.yml\n",
    "conda env create -f insar_analysis_env.yml\n",
    "```\n",
    "Then launch this notebook from the new environment:\n",
    "```shell\n",
    "conda activate insar_analysis\n",
    "jupyter lab igarss_mtedgecumbe_ts_analysis.ipynb\n",
    "```\n",
    "\n",
    "Once you have completed the setup for one of these two environments, you are ready to start working with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Search for SLC images over Haiti in a defined time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2020-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-09-09T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-08-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "import asf_search\n",
    "scenes_to_submit = []\n",
    "wkt = 'POLYGON((-74.6 18.0,-73.0 18.0,-73.0 18.8,-74.6 18.8, -74.6 18.0))'\n",
    "results_descending = asf_search.geo_search(platform=[asf_search.PLATFORM.SENTINEL1], intersectsWith=wkt, processingLevel='SLC', start=start_time, end=end_time, relativeOrbit=142, frame=530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "properties = [result.geojson()['properties'] for result in results_descending]\n",
    "geometries = [shape(result.geojson()['geometry']) for result in results_descending]\n",
    "gdf = gpd.GeoDataFrame(properties, geometry=geometries,crs='EPSG:4326').sort_values('startTime')\n",
    "scenes_to_submit = list(gdf['sceneName'])\n",
    "print(gdf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore(style_kwds=dict(color='black', fill=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_gdf = gdf[['sceneName', 'startTime']].copy()\n",
    "plot_gdf['startTime'] = pd.to_datetime(plot_gdf['startTime'])\n",
    "plot_gdf = plot_gdf.sort_values('startTime').reset_index()\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "ax.scatter(pd.to_datetime(plot_gdf['startTime']), plot_gdf.index, s=5)\n",
    "ax.set(xlabel='Date', ylabel='Image Number')\n",
    "ax.axvline(event_time, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Acquire Data using HyP3 SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Submit jobs from Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "\n",
    "print(f'Submitting {len(scenes_to_submit)} jobs')\n",
    "hyp3 = sdk.HyP3()\n",
    "rtc_jobs = sdk.Batch()\n",
    "for scene in scenes_to_submit:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(granule=scene, resolution=20, name='IGARSS-HAITI-20m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Watch processing and download completed jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir_path = 'haiti_rtcs'\n",
    "rtc_jobs =  hyp3.find_jobs(name='IGARSS-HAITI-20m')\n",
    "rtc_jobs = hyp3.watch(rtc_jobs)\n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "file_list = succeeded_jobs.download_files(location=data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extract zipped products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "from pathlib import Path\n",
    "\n",
    "files = Path('haiti_rtcs').glob('*')\n",
    "for file in files:\n",
    "    sdk.util.extract_zipped_product(file, delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Crop all images to same extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "    \n",
    "    Arg:\n",
    "        file_list: a list of GeoTIFF files\n",
    "    \n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in file_list]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    files_for_mintpy = ['_VV.tif', '_VH.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('haiti_rtcs')\n",
    "files = list(data_dir.glob('**/*_VV.tif'))\n",
    "overlap = get_common_overlap(files)\n",
    "clip_hyp3_products_to_common_overlap(data_dir, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load the geotiffs into Xarray with datetime stamps for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def preprocess(da_orig, file_type: str='vh'):\n",
    "    '''function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI? '''\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({'band_data': file_type}).squeeze()\n",
    "    fname = os.path.basename(da_orig['band_data'].encoding['source'])\n",
    "    time = datetime.strptime(fname[7:22], '%Y%m%dT%H%M%S')\n",
    "    da = da.assign_coords({'time': time})\n",
    "    da = da.expand_dims('time')\n",
    "    da = da.drop_duplicates(dim=['x', 'y'])\n",
    "\n",
    "    return da\n",
    "\n",
    "file_paths = list(Path('haiti_rtcs').glob('*/*_VH_clipped.tif'))\n",
    "rtc_vh = xr.open_mfdataset(paths = file_paths, preprocess = preprocess, chunks = 'auto', engine='rasterio', data_vars='minimal', coords='minimal', concat_dim='time', combine='nested', parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_vh['vh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slice = slice(554420, 691768)\n",
    "y_slice = slice(2066380, 1990281)\n",
    "rtc_vh = rtc_vh.sel(x=x_slice, y=y_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_vh['vh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert RTCs to decibel scale and remove low-valued zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rtc_db = 10*np.log10(rtc_vh['vh'])\n",
    "rtc_db = rtc_db.where(rtc_db > -20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6. Create a temporal average for both pre and post EQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2020-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-09-09T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-08-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "date_bins = [start_time, event_time, end_time]\n",
    "date_bin_labels = ['preevent', 'postevent']\n",
    "\n",
    "rtc_db_grouped = rtc_db.groupby_bins('time', date_bins, labels=date_bin_labels)\n",
    "mean_db = rtc_db_grouped.median(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Mask out areas with slopes less than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dem_stitcher import stitch_dem\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "data_dir_path = Path('haiti_rtcs')\n",
    "\n",
    "dem_file = data_dir_path / 'DEM.tif'\n",
    "bounds = [-74.6, 18.0, -73.0, 18.8]\n",
    "X, p = stitch_dem(bounds,\n",
    "                  dem_name='glo_30',  # Global Copernicus 30 meter resolution DEM\n",
    "                  dst_ellipsoidal_height=False,\n",
    "                  dst_area_or_point='Point')\n",
    "\n",
    "\n",
    "with rasterio.open(dem_file, 'w', **p) as ds:\n",
    "   ds.write(X, 1)\n",
    "   ds.update_tags(AREA_OR_POINT='Point')\n",
    "    \n",
    "with rasterio.open(file_paths[0]) as ds:\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    bounds = ds.bounds\n",
    "    xres = ds.transform[0]\n",
    "    yres = ds.transform[4]\n",
    "    \n",
    "gdal.Warp(str(dem_file), str(dem_file), dstSRS=f'EPSG:{epsg}', dstNodata=0,\n",
    "                  outputBounds=bounds, xRes=xres, yRes=yres, targetAlignedPixels=True, resampleAlg='nearest', format='GTiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "process_dem_file = data_dir_path / 'slope.tif'\n",
    "tmp_file = data_dir_path / 'tmp.tif'\n",
    "gdal.DEMProcessing(destName=str(tmp_file), srcDS=str(dem_file), processing='slope', format='GTiff', slopeFormat='degree')\n",
    "gdal.Translate(destName=str(process_dem_file), srcDS=str(tmp_file), format='GTiff')\n",
    "os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "x_slice = slice(554420, 691768)\n",
    "y_slice = slice(2066380, 1990281)\n",
    "data_dir_path = Path('haiti_rtcs')\n",
    "process_dem_file = data_dir_path / 'slope.tif'\n",
    "slope = xr.open_dataset(process_dem_file, engine='rasterio').squeeze()\n",
    "slope = slope.sel(x=x_slice, y=y_slice)\n",
    "slope_mask = slope['band_data'] > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_db = mean_db.where(slope_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 8. Perform a log difference of the two resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_diff =  mean_db[0] - mean_db[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4)\n",
    "\n",
    "log_diff = log_diff.compute(client=client)\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "plot = ax.imshow(log_diff, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "f.colorbar(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_lim = slice(598507,601974)\n",
    "y_lim = slice(2030321,2024714)\n",
    "roi = log_diff.sel(x=x_lim, y=y_lim)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(10,8))\n",
    "plot = ax1.imshow(roi, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "hist = ax2.hist(roi.values.flatten())\n",
    "# f.colorbar(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 8. Discuss options for thresholding the difference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "log_ratio_plot = ax.imshow(log_diff, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "f.colorbar(log_ratio_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "threshold = 3\n",
    "\n",
    "thresholded_image = log_diff > threshold\n",
    "f, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "log_ratio_plot = ax.imshow(thresholded_image, cmap='Greys', interpolation='none')\n",
    "f.colorbar(log_ratio_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis [conda env:.local-insar_analysis]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
