{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Landslide Hazard Analysis Using SAR\n",
    "In this notebook, you will learn how to generate a landslide density map  with an RTC-based change detection analysis.  You will create multi-temporal stacks of Sentinel-1 data using [XArray](https://docs.xarray.dev/en/stable/) and compare data from before and after an earthquake that triggered landslides.\n",
    "\n",
    "In this tutorial, we will focus mapping landslides triggered by an $M_w$ 7.2 [earthquake in Haiti](https://en.wikipedia.org/wiki/2021_Haiti_earthquake) that occurred on August 14, 2021. The earthquake triggered widespread landslides across the southwestern part of the country, with  least 8,444 landslides triggered across a 2,700 $km^2$ (1,000 sq mi) area [(Zhang et al., 2021)](https://www.sciencedirect.com/science/article/abs/pii/S0169555X22003129).  Rapid response to the landslide and inventory of landslides was further hindered by the arrival of Tropical Storm Grace. The presence of consistent cloud coverage makes SAR data an ideal candidate to detect landslides in the aftermath, since SAR can penetrate through cloud cover.\n",
    "\n",
    "This notebook will show you how to perform the time-series change detection as performed by [Handwerger et al., 2022](https://nhess.copernicus.org/articles/22/753/2022/). This process utilizes OnDemand RTC products from the Alaska Satellite Facility.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Use the HyP3 Python SDK to:\n",
    "   - Request On Demand RTC products from ASF HyP3\n",
    "   - Download the RTC products when they are done processing\n",
    "<br></br>\n",
    "2. And you will use [Xarray](https://docs.xarray.dev/en/stable/) and [Dask](https://docs.dask.org) to:\n",
    "   - Download pre-processed data that we will use for the time series\n",
    "   - Load data with Xarray\n",
    "   - Group data pre-event and post-event and average both stacks\n",
    "   - Perform a log difference between pre-event and post-even scenes\n",
    "   - Plot a histogram of the log difference for a subset of the area\n",
    "   - Reproduce the main landslide heatmap figure from Handwerger et al., 2022\n",
    "\n",
    "---\n",
    "**Note:** This notebook uses staged data to ensure that there is adequate time to step through the RTC analysis, and to discuss the analysis methods. All the code for producing the staged data is present in this notebook. If you would like to learn how to prep the data yourself outside of this workshop. All of the sections related to data prep that we will skip during this workshop will have an asterisk (*) next to their title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Initial Setup\n",
    "\n",
    "To run this notebook, you'll need to be in the `insar_analysis` conda environment within OpenSARLab.\n",
    "\n",
    "Alternatively, you can set up your own environment by running these commands in your shell (you'll need to have [conda](https://docs.conda.io/projects/continuumio-conda/en/latest/user-guide/install/index.html) installed):\n",
    "```shell\n",
    "curl -OL https://raw.githubusercontent.com/ASFOpenSARlab/opensarlab-envs/main/Environment_Configs/insar_analysis_env.yml\n",
    "conda env create -f insar_analysis_env.yml\n",
    "```\n",
    "Then launch this notebook from the new environment:\n",
    "```shell\n",
    "conda activate insar_analysis\n",
    "jupyter lab igarss_landslide_heatmaps.ipynb\n",
    "```\n",
    "\n",
    "Once you have completed the setup for one of these two environments, you are ready to start working with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *1. Search for SLC images over Haiti in a defined time period\n",
    "*You can skip this step if using the prepared data\n",
    "The first step in data preparation is getting a list of scene names for the region of interest a year before the Haiti earthquake to a couple of weeks after the earthquake. Then, you will use the [`asf_search`](https://docs.asf.alaska.edu/asf_search/basics/) python module to find Sentinel-1 scenes that fit those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2020-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-09-09T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-08-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "import asf_search\n",
    "scenes_to_submit = []\n",
    "wkt = 'POLYGON((-74.6 18.0,-73.0 18.0,-73.0 18.8,-74.6 18.8, -74.6 18.0))'\n",
    "results_descending = asf_search.geo_search(platform=[asf_search.PLATFORM.SENTINEL1], intersectsWith=wkt, processingLevel='SLC', start=start_time, end=end_time, relativeOrbit=142, frame=530)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This list of results can then be formated to a list of `sceneNames` to submit to HyP3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "properties = [result.geojson()['properties'] for result in results_descending]\n",
    "geometries = [shape(result.geojson()['geometry']) for result in results_descending]\n",
    "gdf = gpd.GeoDataFrame(properties, geometry=geometries,crs='EPSG:4326').sort_values('startTime')\n",
    "scenes_to_submit = list(gdf['sceneName'])\n",
    "print(gdf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can explore a map of the scene boundaries using the GeoDataFrame explorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore(style_kwds=dict(color='black', fill=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also visualize how many scenes were acquired before and after the earthquake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_gdf = gdf[['sceneName', 'startTime']].copy()\n",
    "plot_gdf['startTime'] = pd.to_datetime(plot_gdf['startTime'])\n",
    "plot_gdf = plot_gdf.sort_values('startTime').reset_index()\n",
    "\n",
    "f, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "ax.scatter(pd.to_datetime(plot_gdf['startTime']), plot_gdf.index, s=5)\n",
    "ax.set(xlabel='Date', ylabel='Image Number')\n",
    "ax.axvline(event_time, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## *2. Acquire Data using HyP3 SDK\n",
    "*You can skip this step if using the prepared data\n",
    "\n",
    "A major step towards working with SAR data at scale is learning how to request and download data programmatically. Accomplishing these tasks via code makes it much easier to request large quantities of data and to make similar requests in the future.\n",
    " To request the generation of Radiometrically Corrected Terrain (RTC) products from ASF, you can follow the general steps outlined in the cells below. For this example, you'll be requesting RTC products for the scenes you searched for in Step 1 to detect landslides in Haiti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1 **Initial HyP3 SDK Setup**\n",
    "If you donâ€™t already have NASA Earthdata Login credentials, create a NASA Earthdata login profile. This will allow you connect to ASF HyP3 via the Python SDK. Since RTC processing can take some time, skip to Step 2.3 to download products that have already finished processsing.\n",
    "Running the cell below will prompt you for your Earthdata username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "hyp3 = sdk.HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.2: **Submit a processing request (Skip this step during seminar)**\n",
    " Once you've logged into the HyP3 API, use the HyP3 SDK's [`submit_rtc_job`](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job) function to submit a processing request for all the scene names prepared in Step 1. You can use the `project_name` name argument to group sets of requests together under one name so that you can easily look them up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Submitting {len(scenes_to_submit)} jobs')\n",
    "rtc_jobs = sdk.Batch()\n",
    "for scene in scenes_to_submit:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(granule=scene, resolution=20, name='IGARSS-HAITI-20m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting your request, ASF will process your data using HyP3, and output an InSAR product. You can monitor the status of your request by calling the [`watch`](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.watch) HyP3 SDK method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rtc_jobs = hyp3.watch(rtc_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.3 **Download succeeded jobs and unzip**\n",
    "Once processing is complete, the RTC job status will be updated to `SUCCEEDED` and data will be ready to download. You can download the data by searching for jobs using a project name and the `SUCCEEDED` status code. Here, we will search for jobs already finished with the job name `IGARSS_HAITI_20m`â€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rtc_jobs =  hyp3.find_jobs(name='IGARSS-HAITI-20m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "â€¦then using the download_files method to download data to a designated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir_path = 'haiti_rtcs'\n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "file_list = succeeded_jobs.download_files(location=data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The HyP3 SDK also includes an extract_zipped_product utility that you can use to unzip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "files = Path('haiti_rtcs').glob('*')\n",
    "for file in files:\n",
    "    sdk.util.extract_zipped_product(file, delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3. Crop all images to a common overlap\n",
    "*You can skip this step if using the prepared data\n",
    "\n",
    "You will need to subset all images to a common overlap to ensure all of your data are overlapping. For the purposes of this notebook, you will be using scenes with `VH` polarizations. Geotiffs are clipped using a [gdal](https://gdal.org/index.html) python binding, [osgeo](https://gdal.org/api/python_bindings.html), which is a straightforward option to utilize geospatial raster functions in your `insar_analysis` conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "    \n",
    "    Arg:\n",
    "        file_list: a list of GeoTIFF files\n",
    "    \n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in file_list]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    files_for_mintpy = ['_VH.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('haiti_rtcs')\n",
    "files = list(data_dir.glob('**/*_VH.tif'))\n",
    "overlap = get_common_overlap(files)\n",
    "clip_hyp3_products_to_common_overlap(data_dir, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4. Prep data for later use\n",
    "*You can skip this step if using the prepared data\n",
    "\n",
    "This step will create the staged data archive that we will download in the next step. This step has no effect on the data we're using, but it's necessary for packaging the data into a zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "save_dir = Path('haiti_rtcs_staged')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "file_paths = Path('haiti_rtcs').glob('*/*_VH_clipped.tif')\n",
    "\n",
    "output = [shutil.copy(file, save_dir) for file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('haiti_rtcs_staged', 'zip', save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download and extract staged data\n",
    "If working with the staged data, this will be the section where you start. In this section we'll download the staged data from Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3_client.download_file('ffwilliams2-shenanigans', 'igarss_2023/haiti_rtcs_staged.zip', 'haiti_rtcs_staged.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('haiti_rtcs_staged.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('haiti_rtcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download and prep auxillary data\n",
    "\n",
    "In addition to our RTC data, we'll also need some additional data. In particular we'll a DEM, a DEM-based slope raster, and a Sentinel-2 optical image to compare our data to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 **DEM-based data**\n",
    "First, we'll download the data [GLO30 DEM](https://spacedata.copernicus.eu/collections/copernicus-digital-elevation-model) data for Haiti. The European Space Administration provides this DEM, and it is the best freely-available global DEM currently available. We'll download it using the `dem_stitcher` Python library which we'll install below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install dem_stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from dem_stitcher import stitch_dem\n",
    "\n",
    "data_dir_path = Path('haiti_rtcs')\n",
    "file_paths = list(Path('haiti_rtcs').glob('*_VH_clipped.tif'))\n",
    "\n",
    "dem_file = data_dir_path / 'DEM.tif'\n",
    "bounds = [-74.6, 18.0, -73.0, 18.8]\n",
    "X, p = stitch_dem(bounds,\n",
    "                  dem_name='glo_30',  # Global Copernicus 30 meter resolution DEM\n",
    "                  dst_ellipsoidal_height=False,\n",
    "                  dst_area_or_point='Point')\n",
    "\n",
    "\n",
    "with rasterio.open(dem_file, 'w', **p) as ds:\n",
    "   ds.write(X, 1)\n",
    "   ds.update_tags(AREA_OR_POINT='Point')\n",
    "    \n",
    "with rasterio.open(file_paths[0]) as ds:\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    # bounds = ds.bounds\n",
    "    xres = ds.transform[0]\n",
    "    yres = ds.transform[4]\n",
    "\n",
    "bounds = [554420, 1990281, 691768, 2066380]\n",
    "    \n",
    "ds = gdal.Warp(str(dem_file), str(dem_file), dstSRS=f'EPSG:{epsg}', dstNodata=0,\n",
    "        outputBounds=bounds, xRes=xres, yRes=yres, targetAlignedPixels=True, resampleAlg='nearest', format='GTiff')\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our DEM is downloaded and in the same coordinate reference system as our RTC data, we'll create a slope raster using GDAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "\n",
    "process_dem_file = data_dir_path / 'slope.tif'\n",
    "tmp_file = data_dir_path / 'tmp.tif'\n",
    "gdal.DEMProcessing(destName=str(tmp_file), srcDS=str(dem_file), processing='slope', format='GTiff', slopeFormat='degree')\n",
    "gdal.Translate(destName=str(process_dem_file), srcDS=str(tmp_file), format='GTiff')\n",
    "os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 **Sentinel-2 data** To have something to compare our analysis to, we'll also download some optical data collected by the Sentinel-2 satellite just after the Haiti earthquake. This data is available on the [AWS Open Data Registry](https://registry.opendata.aws/sentinel-2/). If you have an AWS set up, you can use the commands below to download the data directly.\n",
    "\n",
    "```bash\n",
    "aws s3 cp --request-payer requester s3://sentinel-s2-l2a/tiles/18/Q/WF/2021/9/8/0/R20m/TCI.jp2 s2_haiti_left.jp2\n",
    "aws s3 cp --request-payer requester s3://sentinel-s2-l2a/tiles/18/Q/XF/2021/9/8/0/R20m/TCI.jp2 s2_haiti_right.jp2\n",
    "```\n",
    "\n",
    "For this workshop, you will NOT have an AWS profile set up, so we've staged some data in an open AWS S3 bucket for you to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "s3_client.download_file('ffwilliams2-shenanigans', 'igarss_2023/s2_haiti_left.jp2', 'haiti_rtcs/s2_haiti_left.jp2')\n",
    "s3_client.download_file('ffwilliams2-shenanigans', 'igarss_2023/s2_haiti_right.jp2', 'haiti_rtcs/s2_haiti_right.jp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then similar to our DEM prep, we'll merge the data we downloaded and transform it to the bounds and coordinate reference system of our RTC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "file_paths = list(Path('haiti_rtcs').glob('*_VH_clipped.tif'))\n",
    "\n",
    "with rasterio.open(file_paths[0]) as ds:\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    xres = ds.transform[0]\n",
    "    yres = ds.transform[4]\n",
    "\n",
    "bounds = [554420, 1990281, 691768, 2066380]\n",
    "\n",
    "infiles = ['haiti_rtcs/s2_haiti_left.jp2', 'haiti_rtcs/s2_haiti_right.jp2']\n",
    "outfile = 'haiti_rtcs/s2_haiti.tif'\n",
    "ds = gdal.Warp(outfile, infiles, dstSRS=f'EPSG:{epsg}', dstNodata=0, \n",
    "          outputBounds=bounds, xRes=xres, yRes=yres, targetAlignedPixels=True, resampleAlg='nearest', format='GTiff')\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load the geotiffs into Xarray with datetime stamps for each image\n",
    "After all of your data orginization and preparation, it is finally time to load data with [Xarray](https://docs.xarray.dev/en/stable/getting-started-guide/why-xarray.html), a python module that simplifies working with mutli-dimensional arrays. This will be an excellent tool for working of a stack of raster data, especially since each raster can be indexed by its acquisition date, which we will call the `time` dimension of the stack.\n",
    "Note that Xarray integrates [Dask](https://www.dask.org/) to support [parallel computing](https://docs.xarray.dev/en/stable/user-guide/dask.html), so you will use an argument called `chunks` to instruct Dask on how to divide arrays. This also means that computation will be delayed, so that you will provide it instructions on what to do and later call a `compute` function that will perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def preprocess(da_orig, file_type: str='vh'):\n",
    "    '''function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI? '''\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({'band_data': file_type}).squeeze()\n",
    "    fname = os.path.basename(da_orig['band_data'].encoding['source'])\n",
    "    time = datetime.strptime(fname[7:22], '%Y%m%dT%H%M%S')\n",
    "    da = da.assign_coords({'time': time})\n",
    "    da = da.expand_dims('time')\n",
    "    da = da.drop_duplicates(dim=['x', 'y'])\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_paths = list(Path('haiti_rtcs').glob('*_VH_clipped.tif'))\n",
    "rtc_vh = xr.open_mfdataset(paths = file_paths, preprocess = preprocess, chunks = 'auto', engine='rasterio', data_vars='minimal', coords='minimal', concat_dim='time', combine='nested', parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that your data are loaded in, you can run the next cell to get a visual representation of the stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_vh['vh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the array is a bit larger than 19 GB? Using Xarray's and Dask's chunked computing capabilities allows us to work with datasets that are much larger than we can fit in memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since clipped data are still larger than the area of interest of where the landslides were triggered, you can clip the stacked rasters to the same `x` and `y` slices. This means only the pixels in the AOI will be included in computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slice = slice(554420, 691768)\n",
    "y_slice = slice(2066380, 1990281)\n",
    "rtc_vh = rtc_vh.sel(x=x_slice, y=y_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, you can check that the slicing was successful and has the new dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_vh['vh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert RTCs to decibel scale and remove low-valued zones\n",
    "Next, we'll convert the RTC products from the default output of Power to the decibel (dB) scale. The dB scale is a good scale to use for looking for changes in the landscape, due to its logarithmic scale. Once converted, you will want to mask pixels that are low-values under -20 dB to remove some noise from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "rtc_db = 10 * np.log10(rtc_vh['vh'])\n",
    "rtc_db = rtc_db.where(rtc_db > -20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how that cell completed quickly even though we performed a complex calculation on Gigabytes of data? That's because Xarray and Dask delay performing calculations until we need to use the underlying data. This is useful when performing several operations in a row (like we will) because we'll only need to read the data once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9. Create a temporal average for both pre and post\n",
    "You can now group the RTC stack by scenes acquired before and after the 2021 earthquake that triggered widespread landslides in Haiti. Using the Xarray function [GroupBy](https://docs.xarray.dev/en/stable/user-guide/groupby.html), you can create bins and index according to the event time. Then, you can take the median of the pre-event and post-event stacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2020-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-09-09T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-08-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "date_bins = [start_time, event_time, end_time]\n",
    "date_bin_labels = ['preevent', 'postevent']\n",
    "\n",
    "rtc_db_grouped = rtc_db.groupby_bins('time', date_bins, labels=date_bin_labels)\n",
    "mean_db = rtc_db_grouped.median(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, notice how fast that cell completed because we are delaying computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting groups of data appear now as 2 datasets in the stack: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Mask out areas with slopes less than 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to temporally averaging the data to reduce noise, we're also going to exclude areas from the analysis if we are confident that landslides can't occur there. In this case we'll use the heuristic that *landslides only occur in steep areas and not on flat ground* to remove some areas. We'll do this by using a DEM to calculate the local slope of our study area. Then we'll mask out all areas that have a less than a 5 degree slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read in the data using Xarray and create the < 5 degree slope mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "data_dir_path = Path('haiti_rtcs')\n",
    "process_dem_file = data_dir_path / 'slope.tif'\n",
    "slope = xr.open_dataset(process_dem_file, engine='rasterio').squeeze()\n",
    "slope_mask = slope['band_data'] > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply the mask to our RTC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_db = mean_db.where(slope_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11. Perform a log difference of the two resulting images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's finally time to create the log difference image by subtracting the dB scale pre-earthquake median map from the dB scale post-earthquake median map. Notice how we're calling `compute` on the resulting Xarray object. This tells Xarray to finally perform all of the computations that we've been providing it. This cell should take about two minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "log_diff =  mean_db[0] - mean_db[1]\n",
    "log_diff = log_diff.compute(client=client)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results! Areas with landslide activity should show up as clusters of large positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(10,4))\n",
    "plot = ax.imshow(log_diff, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "f.colorbar(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a close-up look at smaller regions. In the graph below we look at an area that has some landslide activity in the top of the image. The histogram on the right show the distribution of log difference values in this region. As you can see, this distribution looks approximately normal, which means that it may be hard to select classification values when we try to identify discrete landslides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_lim = slice(598507,601974)\n",
    "y_lim = slice(2030321,2024714)\n",
    "roi = log_diff.sel(x=x_lim, y=y_lim)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(10,8))\n",
    "plot = ax1.imshow(roi, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "hist = ax2.hist(roi.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12. Threshold the log difference image to identify landslides\n",
    " As mentioned previously, landslides remove vegetation from the slopes, which will correspond to positive values, since there is an increase in post-event backscatter intensity. By setting a threshold intensity change for landslide areas, pixels with significant change will be included in your final map. First, re-plot the log difference pixels for a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "f, ax = plt.subplots(1,1, figsize=(10,4))\n",
    "log_ratio_plot = ax.imshow(log_diff, cmap='seismic', interpolation='none', vmin=-10, vmax=10)\n",
    "f.colorbar(log_ratio_plot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then plot which pixels exceed the landslide threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(plot_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "threshold = 2.5\n",
    "thresholded_image = log_diff > threshold\n",
    "\n",
    "colors = [(0, 0, 0, 0), (1, 0, 0, 1)]\n",
    "cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(10,4))\n",
    "base_image = ax.imshow(mpimg.imread('haiti_rtcs/s2_haiti.tif'), interpolation='none')\n",
    "landslide_plot = ax.imshow(thresholded_image, cmap=cmap, interpolation='none')\n",
    "settings = ax.set(xticks=[], yticks=[])\n",
    "f.colorbar(landslide_plot, label='Landslide Present')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this map, we used a threshold of 2.5 Db, but this may not be the *best* threshold value. As discussed in Handwerger et al., 2022 selecting an appropriate threshold value is a difficult but important process. Like Handwerger et al., we're using an iterative approach to find the best value possible. Do you think you can find a better value than 2.5 Db?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create the final density plot\n",
    "While the previous map provides an idea of what the spatial distribution of landslides is, we can understand the spatial distribution even better by plotting it as a density map. Warmer colors indicate there are more landslides, whereas green represent regions with fewer pixels exceeding the threshold. You can also compare the epicenter of the August 14 earthquake, represented by the black star, with the distribution of landslides. \n",
    "Note how the epicenter is not in the centered on the area with the darkest red. Rather, these landslides were triggered remotely by the earthquake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create the landslide density data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "thresholded_ndarray = thresholded_image.values.astype(int)\n",
    "thresholded_ndarray = np.round(zoom(thresholded_ndarray, 0.125)).astype(int)\n",
    "\n",
    "nonzero_indices = np.transpose(np.nonzero(thresholded_ndarray))\n",
    "kde = gaussian_kde(nonzero_indices.T, bw_method=0.2)\n",
    "\n",
    "x_indices, y_indices = np.indices(thresholded_ndarray.shape)\n",
    "\n",
    "x_flattened = x_indices.flatten()\n",
    "y_flattened = y_indices.flatten()\n",
    "\n",
    "indices = np.vstack((x_flattened, y_flattened))\n",
    "density_array = kde(indices)\n",
    "density_array = density_array.reshape(thresholded_ndarray.shape)\n",
    "density_array[density_array < 0.0000015] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll resample our DEM into the same resolution as the density data so we can use it as a basemap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "x_slice = slice(554420, 691768)\n",
    "y_slice = slice(2066380, 1990281)\n",
    "dem = xarray.open_dataset('haiti_rtcs/DEM.tif', engine='rasterio').squeeze()\n",
    "dem = dem.sel(x=x_slice, y=y_slice)\n",
    "\n",
    "dem_ndarray = dem['band_data'].values\n",
    "dem_ndarray[np.isnan(dem_ndarray)] = 0\n",
    "dem_ndarray = zoom(dem_ndarray, 0.125)\n",
    "dem_ndarray[dem_ndarray <= 5] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the final plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "colors = [(0, 'lime'),\n",
    "          (0.33, 'yellow'),\n",
    "          (0.6, 'orange'),\n",
    "          (1, 'red')]\n",
    "cmap = LinearSegmentedColormap.from_list('green_orange_red', colors)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "dem_plot = ax1.imshow(dem_ndarray, cmap='Greys', interpolation='none', vmin=-500, vmax=2000)\n",
    "density_plot = ax1.imshow(density_array, cmap=cmap, alpha=0.75, interpolation='none')\n",
    "ax1.annotate('Epicenter', size=10, xy=(662, 165), backgroundcolor='white', zorder=0)\n",
    "ax1.scatter(650, 175, s=250, marker='*', color='black', edgecolor='white')\n",
    "ax1.set(xticks=[], yticks=[])\n",
    "\n",
    "base_image = ax2.imshow(mpimg.imread('haiti_rtcs/s2_haiti.tif'), interpolation='none')\n",
    "ax2.scatter(5200, 1400, s=250, marker='*', color='black', edgecolor='white')\n",
    "ax2.annotate('Epicenter', size=10, xy=(5300, 1300), backgroundcolor='white', zorder=0)\n",
    "ax2.set(xticks=[], yticks=[])\n",
    "                   \n",
    "cax = ax1.inset_axes([0.04, 0.1, 0.3, 0.025])\n",
    "cbar = f.colorbar(density_plot, ax=ax1, cax=cax, label='Landslide Density: Low to High', orientation='horizontal', ticks=[])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to Handwerger et al.'s landslide density plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/handwerger_et_al_figure.png\" alt=\"Handwerger et al. Landslide Density Plot\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis [conda env:.local-insar_analysis]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
