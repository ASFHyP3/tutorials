{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Landslide Hazard Analysis Using SAR\n",
    "Intro to the use case/methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Search for SLC images over Haiti in a defined time period"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.strptime('2017-08-01T23:59', '%Y-%m-%dT%H:%M')\n",
    "end_time = datetime.strptime('2021-08-16T23:59', '%Y-%m-%dT%H:%M')\n",
    "event_time = datetime.strptime('2021-09-14T00:00', '%Y-%m-%dT%H:%M')\n",
    "\n",
    "import asf_search\n",
    "scenes_to_submit = []\n",
    "wkt = 'POLYGON((-74.6 18.0,-73.0 18.0,-73.0 18.8,-74.6 18.8, -74.6 18.0))'\n",
    "results = asf_search.geo_search(platform=[asf_search.PLATFORM.SENTINEL1], intersectsWith=wkt, processingLevel='SLC', start=start_time, end=end_time)\n",
    "[scenes_to_submit.append(result.properties['sceneName']) for result in results]\n",
    "print(f'There are {len(scenes_to_submit)} scenes in the AOI between {start_time} and {end_time}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Request HyP3 processing of Haiti data for two years"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "\n",
    "print(f'Submitting {len(scenes_to_submit)} jobs')\n",
    "hyp3 = sdk.HyP3()\n",
    "rtc_jobs = sdk.Batch()\n",
    "for scene in scenes_to_submit:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(granule=scene, name='IGARSS-HAITI')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rtc_jobs = hyp3.watch(rtc_jobs)\n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "file_list = succeeded_jobs.download_files()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the staged version of the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "for file in file_list:\n",
    "    download_path = Path(file)\n",
    "    s3_client.download_file('jrsmale-shenanigans', f'igarss_2023/{file}', download_path)\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(download_path.stem)\n",
    "    os.unlink(download_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Crop all images to same extent (e.g. InSAR Notebook)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "def clip_hyp3_products_to_same_extent(data_dir: Union[str, Path], extent: List[float]) -> List[str]:\n",
    "    \"\"\"Clip all GeoTIFF hyp3 products to a specified extent\n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        extent:\n",
    "            a list of the upper-left x, upper-left y, lower-right x, and lower-right y corner coordinates of desired extent.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    files_for_xarray = ['VV.tif', 'VH.tif', '_ls_map.tif']\n",
    "\n",
    "    for extension in files_for_xarray:\n",
    "        print(extension)\n",
    "        for file in data_dir.glob(f'**/*{extension}'):\n",
    "            dst_file = Path(str(file.absolute()).replace(f'{extension}', f'{extension[:-4]}_clipped.tif'))\n",
    "            if not Path(dst_file).is_file():\n",
    "                gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=extent)#, projWinSRS=\"EPSG:4326\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = Path('/Users/jrsmale/data/unzipped_IGARSS/')\n",
    "extent = [544823, 2071376, 683411, 1987136]\n",
    "clip_hyp3_products_to_same_extent(data_dir, extent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the geotiffs into Xarray with datetime stamps for each image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tif_names(scene_dir_path):\n",
    "    scene_path_listed = os.listdir(scene_dir_path)\n",
    "    scene_files_vv = [fname for fname in scene_path_listed if fname.endswith('_VV_clipped.tif')]\n",
    "    scene_files_vh = [fname for fname in scene_path_listed if fname.endswith('_VH_clipped.tif')]\n",
    "    scene_files_ls = [fname for fname in scene_path_listed if fname.endswith('_ls_map_clipped.tif')]\n",
    "    scene_file_rm = [fname for fname in scene_path_listed if fname.endswith('README.md.txt')]\n",
    "    return scene_files_vv, scene_files_vh, scene_files_ls, scene_file_rm\n",
    "\n",
    "scenes_listed = os.listdir(data_dir)\n",
    "fpaths_vv, fpaths_vh, fpaths_ls, fpaths_rm = [],[],[], []\n",
    "\n",
    "for element in range(len(scenes_listed)):\n",
    "    print(f'{data_dir}/{scenes_listed[element]}')\n",
    "    try:\n",
    "        good_files = extract_tif_names(f'{data_dir}/{scenes_listed[element]}')\n",
    "        path_vh = f'{data_dir}/{scenes_listed[element]}/{good_files[0][0]}'\n",
    "        path_vv = f'{data_dir}/{scenes_listed[element]}/{good_files[1][0]}'\n",
    "        path_ls = f'{data_dir}/{scenes_listed[element]}/{good_files[2][0]}'\n",
    "        path_readme = f'{data_dir}/{scenes_listed[element]}/{good_files[3][0]}'\n",
    "\n",
    "        fpaths_vv.append(path_vv)\n",
    "        fpaths_vh.append(path_vh)\n",
    "        fpaths_ls.append(path_ls)\n",
    "        fpaths_rm.append(path_readme)\n",
    "    except:\n",
    "        print(f'Couldnt use {scenes_listed[element]}. Skipping.')\n",
    "        continue\n",
    "\n",
    "gdal.BuildVRT(str(data_dir)+'/VV.vrt', fpaths_vv, separate=True)\n",
    "gdal.BuildVRT(str(data_dir)+'/VH.vrt', fpaths_vh, separate=True)\n",
    "gdal.BuildVRT(str(data_dir)+'/ls.vrt', fpaths_ls, separate=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-temporal averaging, create time series plot of single pixel (or group of pixels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "vrt_vv = rio.open_rasterio(str(data_dir)+'/VV.vrt', chunks='auto').squeeze()\n",
    "vrt_vh = rio.open_rasterio(str(data_dir)+'/VH.vrt', chunks='auto').squeeze()\n",
    "vrt_ls = rio.open_rasterio(str(data_dir)+'/ls.vrt', chunks='auto').squeeze()\n",
    "import pandas as pd\n",
    "\n",
    "def extract_metadata_attrs(inputfname):\n",
    "    print(inputfname)\n",
    "    sensor = inputfname[0:3]\n",
    "    beam_mode = inputfname[4:6]\n",
    "    acq_date_full = inputfname[7:22]\n",
    "    acq_datetime = pd.to_datetime(acq_date_full, format='%Y%m%dT%H%M%S')\n",
    "    pol_type = inputfname[23:25]\n",
    "    orbit_type = inputfname[25:26]\n",
    "    orbit_type = inputfname[26:27] #Precise (P), Restituted (R), or Original Predicted (O)\n",
    "    terrain_correction_pixel_spacing = inputfname[27:32] #Terrain Correction Pixel Spacing\n",
    "    rtc_alg = inputfname[33:34] #Software Package Used: GAMMA (G)\n",
    "    output = inputfname[35] #  Gamma-0 (g) or Sigma-0 (s) Output\n",
    "    output_type = inputfname[36] #Power (p) or Decibel (d) or Amplitude (a) Output\n",
    "    masked = inputfname[37]  #Unmasked (u) or Water Masked (w)\n",
    "    filtered = inputfname[38]  # Not Filtered (n) or Filtered (f)\n",
    "    area =  inputfname[39]       # Entire Area (e) or Clipped Area (c)\n",
    "    tbd =   inputfname[40]   #Dead Reckoning (d) or DEM Matching (m)\n",
    "   # product_id  = inputfname[56:62]  #Product ID\n",
    "    product_id = inputfname[42:46]\n",
    "\n",
    "    attrs_dict = { 'sensor': sensor,\n",
    "                        'beam_mode':beam_mode,\n",
    "                        'acquisition_datetime': acq_datetime,\n",
    "                        'polarisation_type': pol_type,\n",
    "                     #  'primary_polarisation': primary_pol,\n",
    "                        'orbit_type': orbit_type,\n",
    "                        'terrain_correction_pixel_spacing' : terrain_correction_pixel_spacing,\n",
    "                        'output_format': output,\n",
    "                        'output_type': output_type,\n",
    "                        'masked' : masked,\n",
    "                        'filtered':filtered,\n",
    "                        'area':area,\n",
    "                        'product_id': product_id\n",
    "                 }\n",
    "    return attrs_dict\n",
    "\n",
    "acq_dates_vh = [extract_metadata_attrs(Path(file).name)['acquisition_datetime'].strftime('%m/%d/%YT%H%M%S') for file in\n",
    "                fpaths_vh]\n",
    "acq_dates_vv = [extract_metadata_attrs(Path(file).name)['acquisition_datetime'].strftime('%m/%d/%YT%H%M%S') for file in\n",
    "                fpaths_vv]\n",
    "acq_dates_ls = [extract_metadata_attrs(Path(file).name)['acquisition_datetime'].strftime('%m/%d/%YT%H%M%S') for file in\n",
    "                fpaths_ls]\n",
    "\n",
    "vrt_vv = vrt_vv.assign_coords({'acq_date':pd.to_datetime(acq_dates_vv, format='%m/%d/%YT%H%M%S')})\n",
    "vrt_vh = vrt_vh.assign_coords({'band':pd.to_datetime(acq_dates_vh, format='%m/%d/%YT%H%M%S')})\n",
    "vrt_ls = vrt_ls.assign_coords({'band':pd.to_datetime(acq_dates_ls, format='%m/%d/%YT%H%M%S')})\n",
    "\n",
    "vrt_merged = xr.Dataset({'vv': vrt_vv,\n",
    "                        'vh': vrt_vh,\n",
    "                        'ls': vrt_ls}).rename_dims({'band': 'acq_date'})\n",
    "\n",
    "vrt_merged = vrt_merged.sortby(vrt_merged.acq_date)\n",
    "\n",
    "meta_attrs_ls = [extract_metadata_attrs(Path(file).name) for file in fpaths_ls]\n",
    "meta_attrs_vv = [extract_metadata_attrs(Path(file).name) for file in fpaths_vv]\n",
    "meta_attrs_vh = [extract_metadata_attrs(Path(file).name) for file in fpaths_vh]\n",
    "\n",
    "import markdown\n",
    "def stacked_meta_tuple(input_ls):\n",
    "    '''takes a list of dictionaries where each dict is metadata for a given time step,\n",
    "    returns a tuple. input list is re-organized to a tuple where each [0] values is a metadata category (ie. sensor) and\n",
    "    [1] is a time series of the categories values over time (ie. S1A, S1A, S1A ...)'''\n",
    "\n",
    "    meta_dict = input_ls[0]\n",
    "    ticker = 0\n",
    "    attrs_dicts, keys_ls = [],[]\n",
    "\n",
    "    for key in meta_dict:\n",
    "        if key == 'acquisition_datetime':\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            key_dict = {f'{key}':[input_ls[ticker][key] for ticker in range(len(input_ls))]}\n",
    "            ticker+=1\n",
    "            attrs_dicts.append(key_dict)\n",
    "            keys_ls.append(key)\n",
    "\n",
    "    full_tuple = tuple(zip(keys_ls, attrs_dicts))\n",
    "\n",
    "    return full_tuple\n",
    "\n",
    "def apply_meta_coords(input_xr, input_tuple):\n",
    "    '''takes an input xarray object and a tuple of metadata created from the above fn.\n",
    "    returns xr object with metadata from tuple applied\n",
    "    '''\n",
    "\n",
    "    out_xr = xr.Dataset(\n",
    "        input_xr.data_vars,\n",
    "        coords = {'x': input_xr.x.data,\n",
    "                  'y': input_xr.y.data,\n",
    "        #          'acq_date': input_xr.acq_date.data,\n",
    "                 }\n",
    "    )\n",
    "    # now apply metadata coords\n",
    "    for element in range(len(input_tuple)):\n",
    "        key = input_tuple[element][0]\n",
    "        coord = list(input_tuple[element][1].values())[0]\n",
    "        out_xr.coords[f'{key}'] = ('acq_date', coord)\n",
    "       # print(key)\n",
    "       # print(out_xr.coords.values)\n",
    "\n",
    "    return out_xr\n",
    "\n",
    "import numpy as np\n",
    "meta_tuple = stacked_meta_tuple(meta_attrs_vv)\n",
    "vrt_full = apply_meta_coords(vrt_merged, meta_tuple)\n",
    "\n",
    "\n",
    "def extract_granule_id(filepath):\n",
    "    ''' takes a filepath to the readme associated with an S1 scene and returns the source granule id used to generate the RTC imagery'''\n",
    "    data = Path(filepath).read_text()\n",
    "    #this text precedes granule ID in readme\n",
    "    gran_str = 'The source granule used to generate the products contained in this folder is:\\n'\n",
    "    split = data.split(gran_str)\n",
    "    #isolate the granule id\n",
    "    gran_id = split[1][:67]\n",
    "\n",
    "    return gran_id\n",
    "\n",
    "def make_granule_coord(readme_fpaths_ls):\n",
    "    '''takes a list of the filepaths to every read me, extracts the granule ID,\n",
    "    extracts acq date for each granule ID, organizes this as an array that\n",
    "    can be assigned as a coord to an xarray object'''\n",
    "\n",
    "    granule_ls = [extract_granule_id(readme_fpaths_ls[element]) for element in range(len(readme_fpaths_ls))]\n",
    "    acq_date = [pd.to_datetime(granule[17:25]) for granule in granule_ls]\n",
    "    granule_da = xr.DataArray(data = granule_ls,\n",
    "                              dims = ['acq_date'],\n",
    "                              coords = {'acq_date':acq_date},\n",
    "                              attrs = {'description': 'source granule ID for ASF-processed S1 RTC imagery, extracted from README files for each scene'},\n",
    "                              name = 'granule_id')\n",
    "    granule_da = granule_da.sortby(granule_da.acq_date)\n",
    "\n",
    "    return granule_da\n",
    "granule_da = make_granule_coord(fpaths_rm)\n",
    "vrt_full.coords[\"granule_id\"] = ('acq_date', granule_da.data)\n",
    "vrt_full =  vrt_full.where(vrt_full.vv != 0., np.nan, drop=False)\n",
    "vrt_full.to_netcdf(str(data_dir)+'/prepared_Haiti_data.nc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a temporal average for both pre and post EQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_event_average = xr.slice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform a log difference of the two resulting images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discuss options for thresholding the difference image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the COP30 DEM to remove change detections in areas with slopes <5 degrees use gdaldem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Plot a histogram for the difference image (might have to subset histogram source to area with equal amounts of landslides and non-landslides)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a figure to interactively view the data and set a threshold using matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}